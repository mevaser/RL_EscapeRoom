
# ğŸ® Reinforcement Learning Escape Room

An interactive multi-room GridWorld simulation showcasing Reinforcement Learning algorithms across four unique challenges. Each room represents a different RL paradigm, combined with puzzle-inspired mechanics and visuals from classic games and movies.

---

## ğŸš€ Project Overview

The **RL Escape Room** is a modular learning environment where an agent progresses through four rooms, each designed with different RL algorithms and increasingly complex tasks:

| Room | Algorithm | Theme | Special Feature |
| ---- | --------- | ----- | ---------------- |
| 1 | **Dynamic Programming (Value Iteration)** | Harry Potter | Snitch collection with state masks |
| 2 | **SARSA (On-Policy Learning)** | WALL-E | Random charging cell placement |
| 3 | **Q-Learning (Off-Policy Learning)** | Squid Game | Sequential shape collection (circle â†’ square â†’ triangle) |
| 4 | **Deep Q-Network (DQN)** | Rush Hour (Driving Cars) | Moving obstacles with path planning |

The entire simulation is built with Python using `pygame`, `gymnasium`, and `PyTorch`.

---

## ğŸ§  Key Concepts Demonstrated

- GridWorld-based RL environment
- Dynamic state-dependent reward systems
- Complex environment features: slippery tiles, prison tiles, dynamic obstacles
- Visualization of training, policies, and Q-values
- Parameter tuning via Tkinter-based GUI before each training
- Fully interactive gameplay after training

---

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ main.py               # Main game loop and room manager
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ rooms/                # Each RL room logic
â”‚   â”œâ”€â”€ room1_dp.py
â”‚   â”œâ”€â”€ room2_sarsa.py
â”‚   â”œâ”€â”€ room3_qlearning.py
â”‚   â””â”€â”€ room4_dqn.py
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ grid_world.py     # Base GridWorld logic
â”‚   â”œâ”€â”€ renderer.py       # Renderer for rooms 1,2,4
â”‚   â””â”€â”€ renderer3_math.py # Special renderer for math-based shapes (room 3)
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ parameter_dialogs.py  # Tkinter parameter input dialogs
â””â”€â”€ assets/images/        # All visual assets per room
```

---

## ğŸ“¦ Installation

1ï¸âƒ£ Clone this repository:

```bash
git clone <your-repo-url>
cd rl-escape-room
```

2ï¸âƒ£ Install dependencies:

```bash
pip install -r requirements.txt
```

âœ… Required packages:

- numpy
- pygame
- matplotlib
- torch
- gymnasium

See `requirements.txt` for exact versions.

---

## ğŸ® Running the Simulation

Simply execute:

```bash
python main.py
```

You'll be guided room-by-room.

âœ… Before entering each room, youâ€™ll be prompted to enter RL hyperparameters via a user-friendly Tkinter GUI.

âœ… Once training completes, you control the agent interactively to test the learned policy.

---

## ğŸ° Room Descriptions

### Room 1: **Dynamic Programming (Harry Potter - Snitch Puzzle)**

- Collect all snitches using Value Iteration.
- Full state space includes a bitmask for snitch collection state.
- Optimal policy is precomputed (model-based).
- Obstacles, slippery tiles and prison tiles included.

### Room 2: **SARSA (WALL-E Charging Room)**

- Agent must collect random charging cells before reaching goal.
- Charging cells are re-randomized every reset.
- Fully on-policy learning using SARSA updates.
- Adaptive exploration during training.

### Room 3: **Q-Learning (Squid Game - Shape Puzzle)**

- Sequential object collection task (circle â†’ square â†’ triangle).
- Q-table includes extended state (current stage).
- Off-policy learning enables faster convergence.
- Shape-based penalties for wrong order collection.

### Room 4: **Deep Q-Network (Rush Hour Driving Cars Puzzle)**

- Moving car obstacles inspired by Rush Hour board game.
- Neural network approximation of Q-values using PyTorch.
- Replay buffer, target networks and soft updates (DQN architecture).
- Smooth difficulty increase via dynamic cars path.

---

## ğŸ¯ Sample Screenshots

| Harry Potter (Room 1) | WALL-E (Room 2) | Squid Game (Room 3) | Rush Hour (Room 4) |
| ---- | ---- | ---- | ---- |
| ![](assets/images/harry_potter.png) | ![](assets/images/battery.jpg) | ![](assets/images/shapes.png) | ![](assets/images/rush_hour.png) |

*(Replace with your actual screenshots based on the images you provided)*

---

## ğŸ§ª Training Visualization

- ğŸ“ˆ Training curves (episode reward)
- ğŸ“Š Policy heatmaps
- ğŸ“‰ Epsilon decay graphs
- ğŸ§­ Policy arrows visualization

---

## ğŸ›  Features Summary

- Fully interactive environment
- Tkinter-based parameter configuration before each training
- Dynamic difficulty via randomized layouts
- Smooth transitions between rooms
- End-of-room popups and full game completion message
- GPU compatible (for DQN training)

---

## âš ï¸ Known Limitations

- All rooms run sequentially; no save/load checkpoints.
- `pygame` rendering loop requires display access.
- DQN training speed may vary based on hardware.

---

## ğŸ’¡ Educational Goals

This project is designed to help students and practitioners:

- Understand the difference between model-based and model-free RL
- Gain intuition for on-policy vs. off-policy methods
- Implement full RL pipelines (train â†’ evaluate â†’ visualize â†’ deploy)
- Tackle increasingly complex reward structures
- Combine game design with AI training flows

---

## ğŸ™ Credits

- Inspired by famous games and movies:  
  *Harry Potter, WALL-E, Squid Game, Rush Hour*

- Developed as part of a reinforcement learning educational showcase.

---

## ğŸ“ License

Open for educational and non-commercial use.  
Feel free to fork, adapt and extend!
